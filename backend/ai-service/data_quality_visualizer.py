"""
Jazz Reviews DB ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ë° ì‹œê°í™” ë„êµ¬
ëˆ„ë½ëœ ë°ì´í„° í•„ë“œë¥¼ ì‹œê°í™”í•˜ì—¬ ë³´ì¶©ì´ í•„ìš”í•œ ë¶€ë¶„ì„ íŒŒì•…
"""

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns
import numpy as np
from supabase import create_client
import os
from dotenv import load_dotenv
import warnings
from datetime import datetime
# í•œê¸€ í°íŠ¸ ì„¤ì •

plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class DataQualityVisualizer:
    def __init__(self):
        """ë°ì´í„° í’ˆì§ˆ ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”"""
        load_dotenv()
        
        # Supabase ì—°ê²°
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        )
        
        self.df = None
        self.analysis_results = {}
        
        print("ğŸ”§ ë°ì´í„° í’ˆì§ˆ ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_data_from_db(self):
        """Supabase DBì—ì„œ ë°ì´í„° ë¡œë“œ"""
        print("ğŸ“¡ Supabase DBì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        try:
            # ì´ ë ˆì½”ë“œ ìˆ˜ í™•ì¸
            count_response = self.supabase.table('critics_review')\
                .select('id', count='exact')\
                .execute()
            
            total_count = count_response.count if hasattr(count_response, 'count') else None
            
            # ì´ ë ˆì½”ë“œ ìˆ˜ì— ë”°ë¼ batch_size ë™ì  ì„¤ì •
            # Supabase API ê¸°ë³¸ limit: 1000ê°œ (ìµœëŒ€ê°’)
            if total_count:
                if total_count <= 100:
                    batch_size = 100  # ì‘ì€ ë°ì´í„°ì…‹: 100ê°œì”©
                elif total_count <= 1000:
                    batch_size = 500  # ì¤‘ê°„ í¬ê¸°: 500ê°œì”©
                else:
                    batch_size = 1000  # í° ë°ì´í„°ì…‹: Supabase API ìµœëŒ€ limit
                
                estimated_pages = (total_count + batch_size - 1) // batch_size
                print(f"  ğŸ“Š ì´ ë ˆì½”ë“œ ìˆ˜: {total_count:,}ê°œ")
                print(f"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,}ê°œ (ì˜ˆìƒ API í˜¸ì¶œ íšŸìˆ˜: {estimated_pages}íšŒ)")
                print(f"  ğŸ’¡ ì°¸ê³ : Supabase API ìµœëŒ€ limitëŠ” 1000ê°œì…ë‹ˆë‹¤")
            else:
                batch_size = 1000
                print(f"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size:,}ê°œ (ê¸°ë³¸ê°’, Supabase API ìµœëŒ€ limit)")
            
            # ëª¨ë“  ë°ì´í„° ë¡œë“œ (í˜ì´ì§€ë„¤ì´ì…˜ ê³ ë ¤)
            all_data = []
            offset = 0
            
            while True:
                print(f"  ğŸ“¦ ë°°ì¹˜ ë¡œë“œ ì‹œì‘: offset={offset}, limit={batch_size}")
                response = self.supabase.table('critics_review')\
                    .select('*')\
                    .limit(batch_size)\
                    .offset(offset)\
                    .execute()
                
                if not response.data:
                    print(f"  âš ï¸ ì‘ë‹µ ë°ì´í„° ì—†ìŒ, ì¢…ë£Œ")
                    break
                
                loaded_count = len(response.data)
                all_data.extend(response.data)
                print(f"  ğŸ“¦ {offset + 1}~{offset + loaded_count} ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ (ì‹¤ì œ: {loaded_count}ê°œ)")
                
                # ë¡œë“œëœ ë°ì´í„°ê°€ ë°°ì¹˜ ì‚¬ì´ì¦ˆë³´ë‹¤ ì‘ìœ¼ë©´ ë§ˆì§€ë§‰ ë°°ì¹˜
                if loaded_count < batch_size:
                    print(f"  âœ… ë§ˆì§€ë§‰ ë°°ì¹˜ ë„ë‹¬ (ë¡œë“œëœ ê°œìˆ˜: {loaded_count} < ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {batch_size})")
                    break
                
                # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì´ë™
                offset += batch_size
            
            self.df = pd.DataFrame(all_data)
            print(f"âœ… ì´ {len(self.df)}ê°œ ë ˆì½”ë“œ ë¡œë“œ ì™„ë£Œ")
            
            return self.df
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_missing_data(self):
        """ëˆ„ë½ ë°ì´í„° ë¶„ì„"""
        if self.df is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        print("\nğŸ“Š ëˆ„ë½ ë°ì´í„° ë¶„ì„ ì‹œì‘...")
        
        # ê¸°ë³¸ í†µê³„
        total_records = len(self.df)
        total_fields = len(self.df.columns)
        
        # album_info í•„ë“œ ë””ë²„ê¹…ì„ ìœ„í•œ ìƒ˜í”Œ í™•ì¸
        if 'album_info' in self.df.columns:
            print("\nğŸ” album_info í•„ë“œ ìƒ˜í”Œ ë°ì´í„° í™•ì¸:")
            non_null_samples = self.df[self.df['album_info'].notna()]['album_info'].head(5)
            print(f"   Nullì´ ì•„ë‹Œ ìƒ˜í”Œ ìˆ˜: {len(non_null_samples)}")
            for idx, val in non_null_samples.items():
                print(f"   ìƒ˜í”Œ {idx}: {str(val)[:100]}...")
            
            # ë¹ˆ JSON ê°ì²´ë‚˜ ì˜ë¯¸ì—†ëŠ” ê°’ í™•ì¸
            print(f"\n   album_info ìƒì„¸ ë¶„ì„:")
            print(f"   - ì „ì²´ ë ˆì½”ë“œ: {len(self.df)}")
            print(f"   - Null ê°’: {self.df['album_info'].isnull().sum()}")
            print(f"   - ë¹ˆ ë¬¸ìì—´: {(self.df['album_info'] == '').sum()}")
            
            # ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ ë¶„ì„
            album_info_str = self.df['album_info'].astype(str)
            print(f"   - 'nan' ë¬¸ìì—´: {(album_info_str == 'nan').sum()}")
            print(f"   - 'null' ë¬¸ìì—´: {(album_info_str.str.lower() == 'null').sum()}")
            print(f"   - ë¹ˆ JSON '{{}}': {(album_info_str.str.strip() == '{}').sum()}")
            print(f"   - ê³µë°±ë§Œ ìˆëŠ” ê°’: {(album_info_str.str.strip() == '').sum()}")
            
            # ìœ íš¨í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            valid_data = album_info_str[
                (album_info_str != 'nan') & 
                (album_info_str.str.strip() != '') & 
                (album_info_str.str.strip() != '{}') &
                (album_info_str.str.lower() != 'null')
            ]
            print(f"   - ìœ íš¨í•œ ë°ì´í„°ë¡œ ë³´ì´ëŠ” ë ˆì½”ë“œ: {len(valid_data)}")
            if len(valid_data) > 0:
                print(f"   - ìœ íš¨í•œ ë°ì´í„° ìƒ˜í”Œ: {valid_data.iloc[0][:100]}...")
        
        # ëˆ„ë½ ë°ì´í„° ê³„ì‚° (None + ë¹ˆ ë¬¸ìì—´ + ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ + ì˜ë¯¸ì—†ëŠ” ê°’)
        missing_stats = {}
        for col in self.df.columns:
            # None ê°’
            null_count = self.df[col].isnull().sum()
            
            # ë¬¸ìì—´ íƒ€ì…ì¸ ê²½ìš° ë” ì •í™•í•œ ì²´í¬
            if self.df[col].dtype == 'object':
                col_str = self.df[col].astype(str)
                
                # ë¹ˆ ë¬¸ìì—´
                empty_string_count = (self.df[col] == '').sum()
                
                # ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´ (strip í›„ ë¹ˆ ë¬¸ìì—´)
                whitespace_count = (col_str.str.strip() == '').sum() - empty_string_count
                
                # ì˜ë¯¸ì—†ëŠ” ê°’ë“¤ (nan ë¬¸ìì—´, null ë¬¸ìì—´, ë¹ˆ JSON ë“±)
                meaningless_values = (
                    (col_str == 'nan').sum() +
                    (col_str.str.lower() == 'null').sum() +
                    (col_str.str.strip() == '{}').sum()
                )
                
                missing_stats[col] = null_count + empty_string_count + whitespace_count + meaningless_values
            else:
                empty_string_count = 0
                whitespace_count = 0
                missing_stats[col] = null_count
        
        missing_stats = pd.Series(missing_stats)
        missing_pct = (missing_stats / total_records) * 100
        
        # ì™„ì„±ë„ ê³„ì‚°
        completeness = 100 - missing_pct
        
        # ê²°ê³¼ ì €ì¥
        self.analysis_results = {
            'total_records': total_records,
            'total_fields': total_fields,
            'missing_stats': missing_stats,
            'missing_pct': missing_pct,
            'completeness': completeness,
            'overall_quality': completeness.mean()
        }
        
        # ì½˜ì†” ì¶œë ¥
        print(f"ğŸ“Š ì „ì²´ í†µê³„:")
        print(f"   ì´ ë ˆì½”ë“œ ìˆ˜: {total_records:,}ê°œ")
        print(f"   ì´ í•„ë“œ ìˆ˜: {total_fields}ê°œ")
        print(f"   ì „ì²´ ë°ì´í„° í¬ì¸íŠ¸: {total_records * total_fields:,}ê°œ")
        print(f"   ì „ì²´ ë°ì´í„° í’ˆì§ˆ: {completeness.mean():.1f}%")
        
        print(f"\nğŸ“ˆ í•„ë“œë³„ ìƒì„¸ ë¶„ì„:")
        for col in self.df.columns:
            missing_count = missing_stats[col]
            missing_percent = missing_pct[col]
            completeness_pct = completeness[col]
            
            # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            if completeness_pct >= 90:
                quality_grade = "ğŸŸ¢ ìš°ìˆ˜"
            elif completeness_pct >= 70:
                quality_grade = "ğŸŸ¡ ë³´í†µ"
            elif completeness_pct >= 50:
                quality_grade = "ğŸŸ  ê°œì„ í•„ìš”"
            else:
                quality_grade = "ğŸ”´ ì‹¬ê°"
            
            print(f"   {col:25} | {missing_count:5}ê°œ ëˆ„ë½ ({missing_percent:5.1f}%) | {completeness_pct:5.1f}% ì™„ì„± | {quality_grade}")
        
        return self.analysis_results
    
    def create_missing_data_heatmap(self):
        """ëˆ„ë½ ë°ì´í„° íˆíŠ¸ë§µ ìƒì„±"""
        if self.df is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ”¥ ëˆ„ë½ ë°ì´í„° íˆíŠ¸ë§µ ìƒì„± ì¤‘...")
        
        plt.figure(figsize=(15, 10))
        
        # ì„œë¸Œí”Œë¡¯ 1: ëˆ„ë½ ë°ì´í„° íˆíŠ¸ë§µ
        plt.subplot(2, 2, 1)
        
        # ëˆ„ë½ ë°ì´í„° ë§ˆìŠ¤í¬ ìƒì„± (ê°œì„ ëœ ë¡œì§ ì‚¬ìš©)
        missing_mask = self.df.isnull().copy()
        for col in self.df.columns:
            if self.df[col].dtype == 'object':
                col_str = self.df[col].astype(str)
                # ë¹ˆ ë¬¸ìì—´, ê³µë°±ë§Œ ìˆëŠ” ë¬¸ìì—´, ì˜ë¯¸ì—†ëŠ” ê°’ ëª¨ë‘ ì²´í¬
                meaningless_mask = (
                    (self.df[col] == '') |
                    (col_str.str.strip() == '') |
                    (col_str == 'nan') |
                    (col_str.str.lower() == 'null') |
                    (col_str.str.strip() == '{}')
                )
                missing_mask[col] = missing_mask[col] | meaningless_mask
        
        # ì´ì§„ ë°ì´í„°ì— ì í•©í•œ ì»¬ëŸ¬ë§µ ì‚¬ìš© (ë…¸ë€ìƒ‰=ëˆ„ë½, ë³´ë¼ìƒ‰=ì¡´ì¬)
        colors = ['#8B00FF', '#FFFF00']  # ë³´ë¼ìƒ‰(ì¡´ì¬=0), ë…¸ë€ìƒ‰(ëˆ„ë½=1)
        cmap = ListedColormap(colors)
        
        # íˆíŠ¸ë§µ ìƒì„± (ëˆ„ë½=1, ì¡´ì¬=0ìœ¼ë¡œ ë³€í™˜)
        heatmap_data = missing_mask.astype(int)
        
        sns.heatmap(
            heatmap_data,
            cbar=True, 
            yticklabels=False,
            cmap=cmap,
            vmin=0,
            vmax=1,
            cbar_kws={'label': 'Missing Data (0=Present, 1=Missing)', 'ticks': [0, 1]}
        )
        plt.title('Missing Data Heatmap\n(Yellow = Missing, Purple = Present)', fontsize=12, fontweight='bold')
        plt.xlabel('Data Fields')
        plt.ylabel('Records')
        
        # ì„œë¸Œí”Œë¡¯ 2: í•„ë“œë³„ ëˆ„ë½ ë¹„ìœ¨
        plt.subplot(2, 2, 2)
        missing_pct = self.analysis_results['missing_pct']
        bars = plt.bar(range(len(missing_pct)), missing_pct.values, color='coral', alpha=0.7)
        plt.title('Missing Data Percentage by Field', fontsize=12, fontweight='bold')
        plt.xlabel('Fields')
        plt.ylabel('Missing Percentage (%)')
        plt.xticks(range(len(missing_pct)), missing_pct.index, rotation=45, ha='right')
        
        # ìƒ‰ìƒìœ¼ë¡œ ì‹¬ê°ë„ í‘œì‹œ
        for i, (bar, pct) in enumerate(zip(bars, missing_pct.values)):
            if pct > 50:
                bar.set_color('red')
            elif pct > 20:
                bar.set_color('orange')
            elif pct > 10:
                bar.set_color('yellow')
            else:
                bar.set_color('green')
        
        # ì„œë¸Œí”Œë¡¯ 3: ë°ì´í„° ì™„ì„±ë„
        plt.subplot(2, 2, 3)
        completeness = self.analysis_results['completeness']
        bars = plt.bar(range(len(completeness)), completeness.values, color='lightgreen', alpha=0.7)
        plt.title('Data Completeness by Field', fontsize=12, fontweight='bold')
        plt.xlabel('Fields')
        plt.ylabel('Completeness (%)')
        plt.xticks(range(len(completeness)), completeness.index, rotation=45, ha='right')
        plt.ylim(0, 100)
        
        # 90% ì´ìƒì€ ì´ˆë¡ìƒ‰, 70% ì´ìƒì€ ë…¸ë€ìƒ‰, ê·¸ ì™¸ëŠ” ë¹¨ê°„ìƒ‰
        for i, (bar, pct) in enumerate(zip(bars, completeness.values)):
            if pct >= 90:
                bar.set_color('green')
            elif pct >= 70:
                bar.set_color('orange')
            else:
                bar.set_color('red')
        
        # ì„œë¸Œí”Œë¡¯ 4: ì „ì²´ ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
        plt.subplot(2, 2, 4)
        overall_quality = self.analysis_results['overall_quality']
        
        # í’ˆì§ˆ ë“±ê¸‰ë³„ ìƒ‰ìƒ
        if overall_quality >= 90:
            color = 'green'
            grade = 'Excellent'
        elif overall_quality >= 70:
            color = 'orange'
            grade = 'Good'
        elif overall_quality >= 50:
            color = 'red'
            grade = 'Poor'
        else:
            color = 'darkred'
            grade = 'Critical'
        
        plt.bar(['Overall Quality'], [overall_quality], color=color, alpha=0.7)
        plt.title(f'Overall Data Quality: {overall_quality:.1f}%\nGrade: {grade}', 
                 fontsize=12, fontweight='bold')
        plt.ylabel('Quality Score (%)')
        plt.ylim(0, 100)
        
        # í’ˆì§ˆ ë“±ê¸‰ í…ìŠ¤íŠ¸ ì¶”ê°€
        plt.text(0, overall_quality + 2, f'{grade}', ha='center', fontweight='bold', fontsize=10)
        
        plt.tight_layout()
        plt.savefig('data_quality_heatmap.png', dpi=300, bbox_inches='tight')
        
        print("âœ… íˆíŠ¸ë§µ ì €ì¥ ì™„ë£Œ: data_quality_heatmap.png")
        plt.show()
    
    def generate_recommendations(self):
        """ë°ì´í„° ë³´ì¶© ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        if self.analysis_results is None:
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\nğŸ’¡ ë°ì´í„° ë³´ì¶© ê¶Œì¥ì‚¬í•­:")
        print("=" * 60)
        
        missing_pct = self.analysis_results['missing_pct']
        completeness = self.analysis_results['completeness']
        
        # ìš°ì„ ìˆœìœ„ë³„ ë¶„ë¥˜
        critical_fields = missing_pct[missing_pct > 50].index.tolist()
        high_priority_fields = missing_pct[(missing_pct > 20) & (missing_pct <= 50)].index.tolist()
        medium_priority_fields = missing_pct[(missing_pct > 10) & (missing_pct <= 20)].index.tolist()
        
        if critical_fields:
            print("ğŸ”´ CRITICAL (50% ì´ìƒ ëˆ„ë½) - ì¦‰ì‹œ ë³´ì¶© í•„ìš”:")
            for field in critical_fields:
                pct = missing_pct[field]
                print(f"   â€¢ {field}: {pct:.1f}% ëˆ„ë½")
        
        if high_priority_fields:
            print("\nğŸŸ  HIGH PRIORITY (20-50% ëˆ„ë½) - ìš°ì„  ë³´ì¶© ê¶Œì¥:")
            for field in high_priority_fields:
                pct = missing_pct[field]
                print(f"   â€¢ {field}: {pct:.1f}% ëˆ„ë½")
        
        if medium_priority_fields:
            print("\nğŸŸ¡ MEDIUM PRIORITY (10-20% ëˆ„ë½) - ì ì§„ì  ê°œì„ :")
            for field in medium_priority_fields:
                pct = missing_pct[field]
                print(f"   â€¢ {field}: {pct:.1f}% ëˆ„ë½")
        
        # ì™„ì„±ë„ê°€ ë†’ì€ í•„ë“œë“¤
        excellent_fields = completeness[completeness >= 95].index.tolist()
        if excellent_fields:
            print(f"\nğŸŸ¢ EXCELLENT (95% ì´ìƒ ì™„ì„±) - ìš°ìˆ˜í•œ ë°ì´í„° í’ˆì§ˆ:")
            for field in excellent_fields:
                pct = completeness[field]
                print(f"   â€¢ {field}: {pct:.1f}% ì™„ì„±")
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­
        overall_quality = self.analysis_results['overall_quality']
        print(f"\nğŸ“Š ì „ì²´ ë°ì´í„° í’ˆì§ˆ: {overall_quality:.1f}%")
        
        if overall_quality >= 90:
            print("ğŸ‰ ë°ì´í„° í’ˆì§ˆì´ ìš°ìˆ˜í•©ë‹ˆë‹¤! ì¶”ê°€ ë³´ì¶©ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        elif overall_quality >= 70:
            print("âœ… ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ì¼ë¶€ í•„ë“œë§Œ ë³´ì¶©í•˜ë©´ ë©ë‹ˆë‹¤.")
        elif overall_quality >= 50:
            print("âš ï¸ ë°ì´í„° í’ˆì§ˆì´ ë³´í†µì…ë‹ˆë‹¤. ìš°ì„ ìˆœìœ„ í•„ë“œë¶€í„° ë³´ì¶©í•˜ì„¸ìš”.")
        else:
            print("ğŸš¨ ë°ì´í„° í’ˆì§ˆì´ ì‹¬ê°í•©ë‹ˆë‹¤! ì „ì²´ì ì¸ ë°ì´í„° ìˆ˜ì§‘ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    def save_analysis_report(self):
        """CSVë¡œ ì‹œê³„ì—´ ë°ì´í„° ì €ì¥ (ë³€í™” ì¶”ì  ìš©ì´)"""
        if self.analysis_results is None:
            print("âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í˜„ì¬ ì‹œê°„ ì •ë³´
        now = datetime.now()
        date_str = now.strftime('%Y-%m-%d')
        
        # CSVë¡œ ì‹œê³„ì—´ ë°ì´í„° ì €ì¥
        csv_history_file = 'data_quality_history.csv'
        
        # CSV í–‰ ë°ì´í„° ìƒì„±
        csv_row = {
            'date': date_str,
            'timestamp': now.isoformat(),
            'total_records': int(self.analysis_results['total_records']),
            'total_fields': int(self.analysis_results['total_fields']),
            'overall_quality': float(self.analysis_results['overall_quality'])
        }
        
        # ê° í•„ë“œë³„ ì™„ì„±ë„ ì¶”ê°€
        for field in self.df.columns:
            csv_row[f'{field}_completeness'] = float(self.analysis_results['completeness'][field])
            csv_row[f'{field}_missing_pct'] = float(self.analysis_results['missing_pct'][field])
        
        # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë”ì™€ í•¨ê»˜ ìƒì„±, ìˆìœ¼ë©´ ì¶”ê°€
        csv_exists = os.path.exists(csv_history_file)
        df_csv = pd.DataFrame([csv_row])
        
        if csv_exists:
            # ê¸°ì¡´ CSV ì½ê¸°
            df_existing = pd.read_csv(csv_history_file)
            # ìƒˆ ë°ì´í„° ì¶”ê°€
            df_combined = pd.concat([df_existing, df_csv], ignore_index=True)
            df_combined.to_csv(csv_history_file, index=False, encoding='utf-8')
        else:
            # ìƒˆ CSV íŒŒì¼ ìƒì„±
            df_csv.to_csv(csv_history_file, index=False, encoding='utf-8')
        
        print(f"âœ… ì‹œê³„ì—´ íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {csv_history_file}")
        
        # ì´ì „ ê²°ê³¼ì™€ ë¹„êµ (ì´ì „ ë¦¬í¬íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
        self.compare_with_previous()
    
    def compare_with_previous(self):
        """ì´ì „ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµí•˜ì—¬ ê°œì„  ì •ë„ í‘œì‹œ"""
        history_file = 'data_quality_history.csv'
        
        if not os.path.exists(history_file):
            print("ğŸ“Š ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ë¹„êµë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        try:
            df_history = pd.read_csv(history_file)
            
            if len(df_history) < 2:
                print("ğŸ“Š ë¹„êµí•  ì´ì „ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ìµœê·¼ 2ê°œ ë¶„ì„ ê²°ê³¼ ë¹„êµ
            latest = df_history.iloc[-1]
            previous = df_history.iloc[-2]
            
            print("\nğŸ“ˆ ë°ì´í„° í’ˆì§ˆ ë³€í™” ì¶”ì´:")
            print("=" * 60)
            
            # ì „ì²´ í’ˆì§ˆ ë¹„êµ
            quality_change = latest['overall_quality'] - previous['overall_quality']
            quality_change_str = f"{quality_change:+.2f}%" if quality_change != 0 else "ë³€í™” ì—†ìŒ"
            quality_symbol = "ğŸ“ˆ" if quality_change > 0 else "ğŸ“‰" if quality_change < 0 else "â¡ï¸"
            print(f"{quality_symbol} ì „ì²´ ë°ì´í„° í’ˆì§ˆ:")
            print(f"   ì´ì „: {previous['overall_quality']:.2f}%")
            print(f"   í˜„ì¬: {latest['overall_quality']:.2f}%")
            print(f"   ë³€í™”: {quality_change_str}")
            
            # ë ˆì½”ë“œ ìˆ˜ ë¹„êµ
            records_change = latest['total_records'] - previous['total_records']
            if records_change != 0:
                print(f"\nğŸ“Š ì´ ë ˆì½”ë“œ ìˆ˜:")
                print(f"   ì´ì „: {previous['total_records']:,}ê°œ")
                print(f"   í˜„ì¬: {latest['total_records']:,}ê°œ")
                print(f"   ë³€í™”: {records_change:+,}ê°œ")
            
            # í•„ë“œë³„ ê°œì„ ë„ í™•ì¸
            print(f"\nğŸ” ì£¼ìš” í•„ë“œë³„ ê°œì„ ë„:")
            field_columns = [col for col in df_history.columns if col.endswith('_completeness')]
            
            improvements = []
            declines = []
            
            for col in field_columns:
                field_name = col.replace('_completeness', '')
                if field_name in ['date', 'timestamp', 'total_records', 'total_fields', 'overall_quality']:
                    continue
                
                current_val = latest[col]
                previous_val = previous[col]
                change = current_val - previous_val
                
                if abs(change) > 0.01:  # 0.01% ì´ìƒ ë³€í™”ê°€ ìˆëŠ” ê²½ìš°ë§Œ í‘œì‹œ
                    if change > 0:
                        improvements.append((field_name, change, current_val, previous_val))
                    else:
                        declines.append((field_name, change, current_val, previous_val))
            
            if improvements:
                print("\n   âœ… ê°œì„ ëœ í•„ë“œ:")
                for field_name, change, current, prev in sorted(improvements, key=lambda x: x[1], reverse=True)[:5]:
                    print(f"      â€¢ {field_name:20} {prev:6.2f}% â†’ {current:6.2f}% ({change:+.2f}%p)")
            
            if declines:
                print("\n   âš ï¸  ì•…í™”ëœ í•„ë“œ:")
                for field_name, change, current, prev in sorted(declines, key=lambda x: x[1])[:5]:
                    print(f"      â€¢ {field_name:20} {prev:6.2f}% â†’ {current:6.2f}% ({change:+.2f}%p)")
            
            if not improvements and not declines:
                print("   ë³€í™” ì—†ìŒ")
            
            print("\n" + "=" * 60)
            
        except Exception as e:
            print(f"âš ï¸  ë¹„êµ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def visualize_timeseries(self):
        """CSV íˆìŠ¤í† ë¦¬ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹œê³„ì—´ ì‹œê°í™”"""
        history_file = 'data_quality_history.csv'
        
        if not os.path.exists(history_file):
            print("âŒ ì‹œê³„ì—´ íˆìŠ¤í† ë¦¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        print("ğŸ“ˆ ì‹œê³„ì—´ ì‹œê°í™” ìƒì„± ì¤‘...")
        
        try:
            # CSV íŒŒì¼ ì½ê¸°
            df_history = pd.read_csv(history_file)
            
            if len(df_history) < 1:
                print("âŒ ì‹œê³„ì—´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
            df_history['datetime'] = pd.to_datetime(df_history['timestamp'])
            df_history = df_history.sort_values('datetime')
            
            # ì „ì²´ í’ˆì§ˆ ì¶”ì´ ê·¸ë˜í”„
            fig = plt.figure(figsize=(20, 12))
            
            # ì„œë¸Œí”Œë¡¯ 1: ì „ì²´ ë°ì´í„° í’ˆì§ˆ ì¶”ì´
            plt.subplot(2, 2, 1)
            plt.plot(df_history['datetime'], df_history['overall_quality'], 
                    marker='o', linewidth=2, markersize=8, color='#2E86AB')
            plt.title('Overall Data Quality Trend', fontsize=14, fontweight='bold')
            plt.xlabel('Date/Time')
            plt.ylabel('Quality Score (%)')
            plt.ylim(0, 100)
            plt.grid(True, alpha=0.3)
            plt.xticks(rotation=45)
            
            # ë ˆì½”ë“œ ìˆ˜ ì¶”ì´
            plt.subplot(2, 2, 2)
            plt.plot(df_history['datetime'], df_history['total_records'], 
                    marker='s', linewidth=2, markersize=8, color='#A23B72')
            plt.title('Total Records Trend', fontsize=14, fontweight='bold')
            plt.xlabel('Date/Time')
            plt.ylabel('Number of Records')
            plt.grid(True, alpha=0.3)
            plt.xticks(rotation=45)
            
            # í•„ë“œë³„ completeness ì¶”ì´ (100%ê°€ ì•„ë‹Œ í•„ë“œë“¤ë§Œ)
            completeness_cols = [col for col in df_history.columns if col.endswith('_completeness')]
            
            # ëª¨ë“  í•„ë“œ ìˆ˜ì§‘ (100%ì¸ í•„ë“œë„ í¬í•¨)
            all_fields = []
            for col in completeness_cols:
                field_name = col.replace('_completeness', '')
                all_fields.append((field_name, col))
            
            # í•„ë“œë³„ ê°œì„ ë„ ë¹„êµ (ëª¨ë“  í•„ë“œ)
            plt.subplot(2, 1, 2)
            if len(df_history) >= 2:
                first_row = df_history.iloc[0]
                last_row = df_history.iloc[-1]
                
                changes = []
                # ëª¨ë“  í•„ë“œì˜ ë³€í™”ìœ¨ ê³„ì‚°
                for field_name, col in all_fields:
                    change = last_row[col] - first_row[col]
                    changes.append((field_name, change, first_row[col], last_row[col]))
                
                if changes:
                    # ë³€í™”ëŸ‰ì´ í° ìˆœì„œë¡œ ì •ë ¬ (ì ˆëŒ€ê°’ ê¸°ì¤€)
                    changes.sort(key=lambda x: abs(x[1]), reverse=True)
                    field_names = [x[0] for x in changes]
                    changes_values = [x[1] for x in changes]
                    first_values = [x[2] for x in changes]
                    last_values = [x[3] for x in changes]
                    
                    x_pos = np.arange(len(field_names))
                    colors = ['green' if c > 0.01 else 'red' if c < -0.01 else 'gray' for c in changes_values]
                    
                    plt.barh(x_pos, changes_values, color=colors, alpha=0.7)
                    plt.yticks(x_pos, field_names, fontsize=9)
                    plt.xlabel('Change in Completeness (%)', fontsize=12)
                    plt.title('All Fields Completeness Change Rate\n(First vs Last Analysis)', fontsize=14, fontweight='bold')
                    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
                    plt.grid(True, alpha=0.3, axis='x')
                    
                    # ê°’ í‘œì‹œ (ëª¨ë“  í•„ë“œ í‘œì‹œ)
                    for i, (change, first, last) in enumerate(zip(changes_values, first_values, last_values)):
                        plt.text(change, i, f' {first:.1f}%â†’{last:.1f}%', 
                                va='center', fontsize=7)
                else:
                    plt.text(0.5, 0.5, 'No data available for comparison', 
                            ha='center', va='center', transform=plt.gca().transAxes)
            else:
                plt.text(0.5, 0.5, 'Need at least 2 data points for comparison', 
                        ha='center', va='center', transform=plt.gca().transAxes)
                plt.title('All Fields Completeness Change Rate\n(First vs Last Analysis)', fontsize=14, fontweight='bold')
            
            # ì™„ì„±ë„ê°€ 100%ê°€ ì•„ë‹Œ í•„ë“œë“¤ë§Œ í•„í„°ë§ (í†µê³„ ìš”ì•½ìš©)
            fields_to_plot = []
            for col in completeness_cols:
                field_name = col.replace('_completeness', '')
                if field_name not in ['id', 'url', 'personnel', 'created_at']:  # í•­ìƒ 100%ì¸ í•„ë“œ ì œì™¸
                    if df_history[col].min() < 100:  # í•œ ë²ˆì´ë¼ë„ 100% ë¯¸ë§Œì¸ í•„ë“œë§Œ
                        fields_to_plot.append((field_name, col))
            
            # í†µê³„ ìš”ì•½ì„ ë³„ë„ ìœ„ì¹˜ì— ë°°ì¹˜
            fig.text(0.5, 0.02, self._generate_summary_text(df_history, fields_to_plot), 
                    ha='center', fontsize=9, family='monospace',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
            
            plt.tight_layout()
            plt.subplots_adjust(bottom=0.15)  # í†µê³„ ìš”ì•½ ê³µê°„ í™•ë³´
            plt.savefig('data_quality_timeseries.png', dpi=300, bbox_inches='tight')
            plt.close()
            print("âœ… ì‹œê³„ì—´ ì‹œê°í™” ì €ì¥ ì™„ë£Œ: data_quality_timeseries.png")
            
        except Exception as e:
            print(f"âŒ ì‹œê³„ì—´ ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    def _generate_summary_text(self, df_history, fields_to_plot):
        """í†µê³„ ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±"""
        summary_lines = []
        summary_lines.append("Time Series Analysis Summary")
        summary_lines.append("=" * 50)
        summary_lines.append(f"Analysis Period: {df_history['datetime'].min().strftime('%Y-%m-%d %H:%M')} ~ {df_history['datetime'].max().strftime('%Y-%m-%d %H:%M')}")
        summary_lines.append(f"Total Analysis Count: {len(df_history)}")
        
        if len(df_history) >= 2:
            first_quality = df_history.iloc[0]['overall_quality']
            last_quality = df_history.iloc[-1]['overall_quality']
            quality_change = last_quality - first_quality
            summary_lines.append("")
            summary_lines.append(f"Overall Quality Change: {first_quality:.2f}% -> {last_quality:.2f}% ({quality_change:+.2f}%p)")
            
            # ê°€ì¥ ê°œì„ ëœ í•„ë“œ
            improvements = []
            for field_name, col in fields_to_plot:
                first_val = df_history.iloc[0][col]
                last_val = df_history.iloc[-1][col]
                change = last_val - first_val
                if change > 0.01:  # 0.01% ì´ìƒ ê°œì„ 
                    improvements.append((field_name, change))
            
            if improvements:
                improvements.sort(key=lambda x: x[1], reverse=True)
                summary_lines.append("")
                summary_lines.append("Top Improved Fields:")
                for field_name, change in improvements[:5]:
                    summary_lines.append(f"  â€¢ {field_name}: +{change:.2f}%p")
        
        return '\n'.join(summary_lines)
    
    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ Jazz Reviews DB ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì‹œì‘")
        print("=" * 60)
        
        # 1. ë°ì´í„° ë¡œë“œ
        data = self.load_data_from_db()
        if data is None or data.empty:
            return
        
        # ëˆ„ë½ ë°ì´í„° ë¶„ì„
        self.analyze_missing_data()
        
        # íˆíŠ¸ë§µ ìƒì„±
        self.create_missing_data_heatmap()

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        self.generate_recommendations()
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        self.save_analysis_report()
        
        # ì‹œê³„ì—´ ì‹œê°í™” ìƒì„±
        self.visualize_timeseries()
        
        print("\nğŸ‰ ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ì™„ë£Œ!")
        print("\nğŸ“ ìƒì„±ëœ íŒŒì¼:")
        print("   â€¢ data_quality_heatmap.png (íˆíŠ¸ë§µ)")
        print("   â€¢ data_quality_timeseries.png (ì‹œê³„ì—´ ì¶”ì´)")
        print("   â€¢ data_quality_history.csv (ì‹œê³„ì—´ íˆìŠ¤í† ë¦¬)")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        visualizer = DataQualityVisualizer()
        visualizer.run_full_analysis()
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
